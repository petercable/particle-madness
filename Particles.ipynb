{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xray\n",
    "import json\n",
    "import cPickle as pickle\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class StreamKey(object):\n",
    "    def __init__(self, subsite, node, sensor, method, stream):\n",
    "        self.subsite = subsite\n",
    "        self.node = node\n",
    "        self.sensor = sensor\n",
    "        self.method = method\n",
    "        self.stream_name = stream\n",
    "\n",
    "    @staticmethod\n",
    "    def from_dict(d):\n",
    "        return StreamKey(d['subsite'], d['node'], d['sensor'], d['method'], d['stream'])\n",
    "\n",
    "    @staticmethod\n",
    "    def from_refdes(refdes):\n",
    "        return StreamKey(*refdes.split('|'))\n",
    "\n",
    "    @staticmethod\n",
    "    def from_stream_key(stream_key, sensor, stream):\n",
    "        return StreamKey(stream_key.subsite, stream_key.node, sensor, stream_key.method, stream)\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return all([self.subsite == other.subsite,\n",
    "                    self.node == other.node,\n",
    "                    self.sensor == other.sensor,\n",
    "                    self.method == other.method,\n",
    "                    self.stream == other.stream])\n",
    "\n",
    "    def as_dict(self):\n",
    "        return {\n",
    "            'subsite': self.subsite,\n",
    "            'node': self.node,\n",
    "            'sensor': self.sensor,\n",
    "            'method': self.method,\n",
    "            'stream': self.stream_name\n",
    "        }\n",
    "\n",
    "    def as_refdes(self):\n",
    "        return '%(subsite)s|%(node)s|%(sensor)s|%(method)s|%(stream)s' % self.as_dict()\n",
    "\n",
    "    def as_dashed_refdes(self):\n",
    "        return self.as_refdes().replace('|', '-')\n",
    "\n",
    "    def as_three_part_refdes(self):\n",
    "        return \"{:s}-{:s}-{:s}\".format(self.subsite, self.node, self.sensor)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.as_dict())\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.as_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NumpyJSONEncoder(json.JSONEncoder):\n",
    "    \"\"\"\n",
    "    numpy array indexing will often return numpy scalars, for\n",
    "    example a = array([0.5]), type(a[0]) will be numpy.float64.\n",
    "    The problem is that numpy types are not json serializable.\n",
    "    However, they have a lot of the same methods as ndarrays, so\n",
    "    for example, tolist() can be called on a numpy scalar or\n",
    "    numpy ndarray to convert to regular python types.\n",
    "    \"\"\"\n",
    "    def default(self, o):\n",
    "        if isinstance(o, (np.generic, np.ndarray)):\n",
    "            return o.tolist()\n",
    "        else:\n",
    "            return json.JSONEncoder.default(self, o)\n",
    "\n",
    "def return_as_pickled_xray_full(ds):\n",
    "    return pickle.dumps(ds, protocol=-1)\n",
    "\n",
    "def return_as_pickled_xray_min(ds):\n",
    "    return pickle.dumps(ds)\n",
    "\n",
    "def particles(ds, stream_key, parameters):\n",
    "    # convert data into a list of particles\n",
    "    particles = []\n",
    "    for index in xrange(len(ds.time)):\n",
    "        particle = OrderedDict()\n",
    "        \n",
    "        particle['pk'] = stream_key.as_dict()\n",
    "        # Add non-param data to particle\n",
    "        particle['pk']['deployment'] = ds['deployment'].values[index]\n",
    "        particle['pk']['time'] = ds['time'].values[index]\n",
    "        particle['provenance'] = str(ds['provenance'].values[index])\n",
    "\n",
    "        for param in ds.variables:\n",
    "            particle[param] = ds[param].values[index]\n",
    "\n",
    "#             qc_postfixes = ['qc_results', 'qc_executed']\n",
    "#             for qc_postfix in qc_postfixes:\n",
    "#                 qc_key = '%s_%s' % (param, qc_postfix)\n",
    "#                 if qc_key in ds:\n",
    "#                     particle[qc_key] = ds[qc_key].values[index]\n",
    "        particles.append(particle)\n",
    "    return particles\n",
    "\n",
    "def small_particles(ds, stream_key, parameters):\n",
    "    # convert data into a list of particles\n",
    "    particles = []\n",
    "    warned = set()\n",
    "    for index in xrange(len(ds.time)):\n",
    "        particle = OrderedDict()\n",
    "        particle['provenance'] = str(ds['provenance'].values[index])\n",
    "\n",
    "        for param in ds.variables:\n",
    "            particle[param] = ds[param].values[index]\n",
    "\n",
    "            qc_postfixes = ['qc_results', 'qc_executed']\n",
    "            for qc_postfix in qc_postfixes:\n",
    "                qc_key = '%s_%s' % (param, qc_postfix)\n",
    "                if qc_key in ds:\n",
    "                    particle[qc_key] = ds[qc_key].values[index]\n",
    "        particles.append(particle)\n",
    "    return particles\n",
    "\n",
    "def more_particles(ds, stream_key):\n",
    "    particles = []\n",
    "    for index in xrange(len(ds.time)):\n",
    "        particle = {p: ds[p].values[index] for p in ds.variables}\n",
    "        \n",
    "        particle['pk'] = stream_key.as_dict()\n",
    "        # Add non-param data to particle\n",
    "        particle['pk']['deployment'] = ds['deployment'].values[index]\n",
    "        particle['pk']['time'] = ds['time'].values[index]\n",
    "        particle['provenance'] = str(ds['provenance'].values[index])\n",
    "        particles.append(particle)\n",
    "    return particles\n",
    "\n",
    "def particles_json(ds, sk, pa):\n",
    "    return json.dumps(particles(ds, sk, pa), cls=NumpyJSONEncoder)\n",
    "\n",
    "def small_particles_json(ds, sk, pa):\n",
    "    return json.dumps(small_particles(ds, sk, pa), cls=NumpyJSONEncoder)\n",
    "\n",
    "def pete(ds, stream_key):\n",
    "    # convert data into a list of particles\n",
    "    particles = []\n",
    "    data = {}\n",
    "    for param in ds.variables:\n",
    "        data[param] = ds[param].values\n",
    "    for index in ds.index.values:\n",
    "        particle = {p: data[p][index] for p in ds.variables}\n",
    "        particles.append(particle)\n",
    "    return particles\n",
    "\n",
    "def m1(ds, count):\n",
    "    for i in xrange(count):\n",
    "        x = ds.time.values[i]\n",
    "        \n",
    "def m2(ds, count):\n",
    "    t = ds.time.values\n",
    "    for i in xrange(count):\n",
    "        t[i]\n",
    "        \n",
    "def ds_to_dict(ds):\n",
    "    return {p: list(ds[p].values) for p in ds.variables}\n",
    "\n",
    "def to_csv(ds):\n",
    "    ds.to_dataframe().to_csv()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameters = ['time', 'conductivity', 'temperature', 'deployment', 'provenance']\n",
    "with xray.open_dataset('ctdbp_no_sample_0000.nc', decode_times=False) as orig_ds:\n",
    "    orig_ds.load()\n",
    "    full_ds = orig_ds.copy()\n",
    "    ds = xray.Dataset()\n",
    "    for var in parameters:\n",
    "        ds[var] = orig_ds[var][:1000]\n",
    "\n",
    "    \n",
    "sk = StreamKey('a', 'b', 'c', 'd', 'e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 411 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit return_as_pickled_xray_full(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 400 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit particles_json(ds, sk, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 339 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit small_particles_json(ds, sk, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 389 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit particles(ds, sk, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 326 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit small_particles(ds, sk, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 5.43 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit pete(ds, sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 39.2 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit m1(ds, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loops, best of 3: 142 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit m2(ds, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 335 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit more_particles(ds, sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parts = particles(ds, sk, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 26.9 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit json.dumps(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = json.dumps(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 10.5 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit json.loads(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 76.3 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit pickle.dumps(parts, protocol=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 3: 479 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit ds_to_dict(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = ds_to_dict(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 30.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit pickle.dumps(d, protocol=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257394, 52730)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(json.dumps(parts)), len(json.dumps(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loops, best of 3: 1.59 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit to_csv(full_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
